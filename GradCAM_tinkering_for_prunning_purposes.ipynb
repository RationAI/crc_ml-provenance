{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rODEqYrP298"
   },
   "source": [
    "Necessary imports for the whole notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch \n",
    "#%pip install numpy pandas\n",
    "#%pip install matplotlib tqdm\n",
    "#%pip install opencv-python scikit-image\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6GaHSijPEj6",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "#from numba import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtU3Mv9KQ6V-"
   },
   "source": [
    "## Load the configurable components\n",
    "\n",
    "First open the config file with all the necessary configurable components described.\n",
    "Contains mosty data generators as other componentes still need to be rewritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('prunning_teacher_student/prunning_ts_config.json') as ts_config_file:\n",
    "    ts_config = json.load(ts_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the basic data generators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATTZHWIwQ412"
   },
   "outputs": [],
   "source": [
    "from rationai.utils.config import build_from_config, parse_configs_recursively\n",
    "\n",
    "\n",
    "datagen_bool = build_from_config(parse_configs_recursively(\"datagen_bool\", cfg_store=ts_config[\"named_configs\"]))\n",
    "generators_dict_bool = datagen_bool.build_from_template()\n",
    "\n",
    "train_generator_bool = generators_dict_bool['train_gen']\n",
    "valid_generator_bool = generators_dict_bool['valid_gen']\n",
    "\n",
    "#datagen_ts = build_from_config(parse_configs_recursively(\"datagen_ts\", cfg_store=ts_config[\"named_configs\"]))\n",
    "#generators_dict_ts = datagen_ts.build_from_template()\n",
    "\n",
    "#train_generator_ts = generators_dict_ts['train_gen']\n",
    "#valid_generator_ts = generators_dict_ts['valid_gen']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_generator_bool.set_batch_size(batch_size)\n",
    "valid_generator_bool.set_batch_size(batch_size)\n",
    "# train_generator_ts.set_batch_size(batch_size)\n",
    "# valid_generator_ts.set_batch_size(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rationai.utils.config import build_from_config, parse_configs_recursively\n",
    "\n",
    "datagen_seq = build_from_config(parse_configs_recursively(\"datagen_seq\", cfg_store=ts_config[\"named_configs\"]))\n",
    "generators_dict_seq = datagen_seq.build_from_template()\n",
    "seq_generator = generators_dict_seq['just_gen_seq']\n",
    "seq_generator.set_batch_size(1)\n",
    "for i in range(4):\n",
    "    seq_generator.sampler.next()\n",
    "seq_generator.on_epoch_end()\n",
    "print(len(seq_generator))  # 874\n",
    "# indexes around 59 contain the critical segment\n",
    "special_input_, special_output_ = seq_generator[59]\n",
    "special_img = special_input_.squeeze().permute([1,2,0]).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V72n9YQZPuLo"
   },
   "source": [
    "## Modules setup\n",
    "\n",
    "Let's import a GradCam module, defined in a separate file (for simplicity). The module should be able to be used with an arbitrary network topology that contains convolutional layers.\n",
    "The idea of gradcam is that it looks into a model at the particular layer and weights the featrue map activations of the particular layer by the gradients flowing back from the following layers. Averaged together these weighted activation maps create a visual representation of what the network pays attention to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzrwVOn2P15Q"
   },
   "outputs": [],
   "source": [
    "from prunning_gradcam.grad_cam import GradCam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier, the GradCam module is written in such a way that it takes two modules on initialisation, which represent the two parts of the already dissected model. The model dissection has to be performed manually,\n",
    "though I have provided a method for dissecting any torch.nn.Sequential model on an arbitrary layer index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggEhBL6UnjxY"
   },
   "outputs": [],
   "source": [
    "from prunning_gradcam.models import SequentialVGG16\n",
    "from prunning_gradcam.grad_cam_tools import _load_params, dissect_sequential_model\n",
    "\n",
    "import logging as log\n",
    "\n",
    "# initialize the VGG model\n",
    "vgg = SequentialVGG16()\n",
    "\n",
    "# load weights from a file\n",
    "_load_params(vgg, source_state_dict='transplanted-model.chkpt')\n",
    "\n",
    "# dissect model into two parts\n",
    "first_part, remaining_part = dissect_sequential_model(vgg, 29)\n",
    "remaining_part_without_sigmoid, _ = dissect_sequential_model(remaining_part, -1)\n",
    "\n",
    "grad_cam = GradCam(first_part, remaining_part)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math\n",
    "\n",
    "In the original paper, authors calculate average weighted activation in a particular layer. The neuron importance factor $\\alpha_k^c$ is obtained as an average gradient of the class activation $y^c$ (before the last activation function is applied) and each activation map $A^k \\in \\mathbb{R}^{u\\times v} $ is weighted by it.\n",
    "\n",
    "$$\n",
    "\\alpha_k^c = \\frac{1}{Z}\\sum_{i=0}^{u}\\sum_{j=0}^{v} \\frac{\\partial \\xi^c}{\\partial A^k_{ij}}\n",
    "$$\n",
    "$$\n",
    "L^c_{Grad-CAM}=ReLU(\\sum_{k}\\alpha^c_kA^k)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\xi^c_i}{\\partial A_{ki}} = \\frac{\\partial \\xi^c_i}{\\partial y^k_i}  \\frac{\\partial y^k_i}{\\partial A_{ki}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial }{\\partial }\n",
    "$$\n",
    "\n",
    "Since our VGG16 uses global max pooling (GMP), the importance coefficient $\\alpha_k^c$ will result in the same ranking up to a constant factor $k=uv$ for average pooling and for maxpooling, allowing us to also calculate\n",
    "\n",
    "$$\n",
    "\\alpha_k^c = max_{ij}\\frac{\\partial y^c}{\\partial A^k_{ij}}\n",
    "$$\n",
    "\n",
    "which is a little faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualisation tools\n",
    "\n",
    "For the purpose of visualizing the GradCAM outputs we have to have a way of plotting the heatmaps, overlaying them over the original images or somehow visualizing the activation regions.\n",
    "\n",
    "I have coded a function that can overlay several heatmaps over a single image in one step, next to each other in a grid. \n",
    "The last column in the grid shows all the heatmaps over each other clipped.\n",
    "\n",
    "The code is split into multiple helper functions for modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "from skimage.morphology import dilation, disk\n",
    "\n",
    "\n",
    "def _COLORMAP_JETFROMBLACK():\n",
    "    \"\"\"Precomputes a lookup table for a custom colormap\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A numpy array of shape (256,1,3) or something like that.\n",
    "    \"\"\"\n",
    "    lut = np.asarray([[np.ones(3)*i for _ in range(1)] for i in range(256)], dtype=np.uint8)\n",
    "    lut = cv2.applyColorMap(lut, cv2.COLORMAP_JET)\n",
    "    #lut = cv2.cvtColor(lut, cv2.COLOR_BGR2RGB)\n",
    "    beginning = lut[:64, 0].astype(np.float64)\n",
    "    beginning *= np.stack([np.arange(64) * (1/64) for _ in range(3)], axis=1)\n",
    "    lut[:64, 0] = beginning.astype(np.uint8)\n",
    "    plt.imshow(np.swapaxes(np.repeat(lut, 10, axis=1), 0, 1))\n",
    "    plt.title(\"JETFROMBLACK\")\n",
    "    plt.show()\n",
    "    return lut\n",
    "\n",
    "def _COLORMAP_TWILIGHTSYMETRIC():\n",
    "    \"\"\"Precomputes a lookup table for a custom colormap\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A numpy array of shape (256,1,3) or something like that.\n",
    "    \"\"\"\n",
    "    lut = np.asarray([[np.ones(3)*i for _ in range(1)] for i in range(256)], dtype=np.uint8)\n",
    "    lut = cv2.applyColorMap(lut, cv2.COLORMAP_TWILIGHT)\n",
    "    #lut = cv2.cvtColor(lut, cv2.COLOR_BGR2RGB)\n",
    "    beginning = lut[:64, 0].astype(np.float64)\n",
    "    beginning *= np.stack([np.ones(64), np.arange(64) * (1/64), np.arange(64) * (1/64)], axis=1)\n",
    "    lut[:64, 0] = beginning.astype(np.uint8)\n",
    "\n",
    "    middle = lut[96:160, 0].astype(np.float64)\n",
    "    middle *= np.stack([np.sqrt(np.absolute(np.linspace(-1, 1, 64))) for _ in range(3)], axis=1)\n",
    "    lut[96:160, 0] = middle.astype(np.uint8)\n",
    "\n",
    "    #print(lut[127:130])\n",
    "    plt.imshow(np.swapaxes(np.repeat(lut, 10, axis=1), 0, 1))\n",
    "    plt.title(\"TWILIGHTSYMETRIC\")\n",
    "    plt.show()\n",
    "    return lut\n",
    "\n",
    "# return a colormap containing gradient from green to red with zeros in the middle\n",
    "def _COLORMAP_GRADIENT_BIPOLAR_GREEN_YELLOW():\n",
    "    lut_red = np.asarray([[np.asarray([0, 0, i]) for _ in range(1)] for i in range(0, 256, 2)], dtype=np.uint8)\n",
    "    lut_yellow = np.asarray([[np.asarray([0, i, 0]) for _ in range(1)] for i in range(0, 256, 2)], dtype=np.uint8)\n",
    "    lut = np.concatenate((lut_red[::-1], lut_yellow), axis=0)\n",
    "    plt.imshow(np.swapaxes(np.repeat(lut, 10, axis=1), 0, 1))\n",
    "    plt.title(\"GRADIENT_BIPOLAR_GREEN_RED\")\n",
    "    plt.show()\n",
    "    return lut\n",
    "\n",
    "\n",
    "\n",
    "# precomputed stuff like color LUTs and so on\n",
    "_COLORMAP_JETFROMBLACK = _COLORMAP_JETFROMBLACK()\n",
    "_COLORMAP_TWILIGHTSYMETRIC = _COLORMAP_TWILIGHTSYMETRIC()\n",
    "_COLORMAP_GRADIENT_BIPOLAR_GREEN_RED = _COLORMAP_GRADIENT_BIPOLAR_GREEN_YELLOW()\n",
    "\n",
    "\n",
    "def scaleshift(img: np.ndarray, shift:float=None, scale:float=None):\n",
    "    \"\"\"Function to scale and shift real values in a tensor.\n",
    "    By default scaleshifts by min and max to obtain values in the range [0..1]\n",
    "    Creates a copy of the scaled tensor and returns it.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Scaled tensor\n",
    "        shift (float, optional): This value is added to each element of the tensor prior to scaling. Defaults to None.\n",
    "        scale (float, optional): All values are multiplied by this value after shifting. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: New tensor with shifted and scaled values.\n",
    "    \"\"\"\n",
    "    if shift is None:\n",
    "        shift = -img.min()\n",
    "    if scale is None:\n",
    "        scale = 1/img_max\n",
    "\n",
    "    # create a new picture tensor copy by not using inplace addition\n",
    "    img = img + shift  \n",
    "    img_max = img.max()\n",
    "    if img_max != 0:\n",
    "        img *= scale\n",
    "    return img\n",
    "\n",
    "\n",
    "def resize_and_color(bitmap: np.ndarray, dims:Tuple[int], colormap_lut: np.ndarray=_COLORMAP_JETFROMBLACK) -> np.ndarray:\n",
    "    overlay = cv2.resize(bitmap, dims)\n",
    "    overlay = np.uint8(255 * overlay)\n",
    "    overlay = cv2.cvtColor(overlay, cv2.COLOR_GRAY2RGB)\n",
    "    overlay = cv2.LUT(overlay, colormap_lut)\n",
    "    return overlay\n",
    "\n",
    "\n",
    "def superimpose(image: np.ndarray, overlay: np.ndarray, strategy:str='ceil'):\n",
    "    if strategy=='ceil':\n",
    "        res = np.minimum(image + overlay*0.375, 255)\n",
    "    elif strategy=='sub':\n",
    "        res = np.maximum(image - overlay*0.5, 0)\n",
    "    elif strategy=='linear_combination':\n",
    "        res = image*0.5 + overlay*0.5\n",
    "    elif strategy=='outline':\n",
    "        footprint = disk(3)\n",
    "        bool_map = overlay.sum(axis=2) > 0\n",
    "        dilated = dilation(bool_map, footprint)\n",
    "        outline = dilated & ~bool_map\n",
    "        res = image.copy()\n",
    "        # fill the outline with specific color\n",
    "        res[outline] = [0, 0, 0]\n",
    "    elif strategy=='black_as_alpha':\n",
    "        # use custom alpha blending to combine the two images\n",
    "        alpha = np.divide(np.max(overlay, axis=2, keepdims=True), 255.)\n",
    "        res = image*(1-alpha) + overlay*alpha\n",
    "    else:\n",
    "        raise NotImplementedError(f'There is no strategy with name {strategy}!')\n",
    "\n",
    "    return res.astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_bitmap_overlays(bitmaps: List[np.ndarray], base_image: np.ndarray, save_file:str=None, colormap_lut: np.ndarray=_COLORMAP_GRADIENT_BIPOLAR_GREEN_RED, strategy='black_as_alpha', heatmaps_titles=None):\n",
    "    \"\"\"\n",
    "    Creates a grid plot from a list of overlays and a base image.\n",
    "    Expects the overlay arrays in the list to be of shape [W1, H1] and the image of shape [3, W2, H2],\n",
    "    where W1, H1, W2, H2 are widths and heights, not required to be of same size. Overlays are stretched to the image size.\n",
    "    \n",
    "    There are several rows plotted, leftmost column contain teh base images, rightmost column contains all overlays.\n",
    "    Images in in columns in between represent each overlay separately\n",
    "\n",
    "    Args:\n",
    "        bitmaps (List[np.ndarray]): The bitmaps that are going to be transformed into overlays\n",
    "        base_image (np.ndarray): The underlying image\n",
    "        save_file (str, optional): If there is a string present, the figure is saved into a file. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, ncols=len(bitmaps)+1, sharex=True, sharey=True, figsize=(10*(len(bitmaps)+1), 10*2))\n",
    "   \n",
    "    # mark the row indices for flexibility\n",
    "    overlay_row = 0\n",
    "    superimposed_row = 1\n",
    "\n",
    "    # in the first column, just print the original images. The upper is clipped, the lower is rescaled, both to [0..1]\n",
    "    ax[overlay_row,0].imshow(base_image)\n",
    "    ax[overlay_row,0].title.set_text('Base image')\n",
    "    #base_image = scaleshift_to_unit_range(base_image)\n",
    "    colorscale = np.swapaxes(np.repeat(colormap_lut, 10, axis=1), 0, 1)\n",
    "    #ax[superimposed_row][0].imshow(cv2.resize(colorscale, base_image.shape[:2]))\n",
    "    #ax[superimposed_row][0].title.set_text('Heatmap overlay colorscale')\n",
    "\n",
    "    # extract the ahred tick Grouper objcts from the axes and remove the first column axes\n",
    "    axLB = ax[superimposed_row,0]\n",
    "    axLB.get_shared_x_axes().remove(axLB)\n",
    "    axLB.get_shared_y_axes().remove(axLB)\n",
    "    #axLB.clear()\n",
    "\n",
    "    # Create and assign new ticker\n",
    "    xticker = matplotlib.axis.Ticker()\n",
    "    yticker = matplotlib.axis.Ticker()\n",
    "    axLB.xaxis.major = xticker\n",
    "    axLB.yaxis.major = yticker\n",
    "\n",
    "    # The new ticker needs new locator and formatters\n",
    "    xloc = matplotlib.ticker.AutoLocator()\n",
    "    xfmt = matplotlib.ticker.StrMethodFormatter('{x:,.2f}')\n",
    "\n",
    "    yloc = matplotlib.ticker.AutoLocator()\n",
    "    yfmt = matplotlib.ticker.ScalarFormatter()\n",
    "\n",
    "    # Assign the locators and formatters to the axes\n",
    "    axLB.xaxis.set_major_locator(xloc)\n",
    "    axLB.xaxis.set_major_formatter(xfmt)\n",
    "    axLB.yaxis.set_major_locator(yloc)\n",
    "    axLB.yaxis.set_major_formatter(yfmt)\n",
    "\n",
    "    axLB.imshow(cv2.resize(colorscale, base_image.shape[:2]))\n",
    "    axLB.title.set_text('Heatmap overlay colorscale')\n",
    "\n",
    "    # set the new x-axis tick labels to the logit values of the colormap\n",
    "    axLB.set_xticks(np.linspace(0, 512, 10, endpoint=False, dtype=np.int32))\n",
    "    axLB.set_xticklabels(expit(np.linspace(-1, 1, 10, endpoint=False)))\n",
    "\n",
    "    #set yticks empty\n",
    "    axLB.set_yticks([])\n",
    "\n",
    "    \n",
    "    base_image_255 = (base_image*255)\n",
    "    \n",
    "    # for each bitmaps get the overlays and plot them\n",
    "    for column, bitmap in enumerate(iterable=bitmaps, start=1):\n",
    "        overlay = resize_and_color(bitmap, base_image.shape[:2], colormap_lut=colormap_lut)\n",
    "        ax[overlay_row][column].imshow(overlay)  # show the separate overlay on the first row\n",
    "        if heatmaps_titles is not None:\n",
    "            ax[overlay_row][column].title.set_text(heatmaps_titles[column-1])\n",
    "\n",
    "        # impose the overlay on top of the base image\n",
    "        ax[superimposed_row][column].imshow(superimpose(base_image_255, overlay, strategy))  # show the superimposed image on the second row\n",
    "        \n",
    "    # # get the total averaged overlay from all the bitmaps\n",
    "    # overlay = resize_and_color(np.minimum(sum(bitmaps), 1.0), base_image.shape[:2], colormap_lut=colormap_lut)\n",
    "    # ax[overlay_row][-1].imshow(overlay)  # show the separate overlay on the first row\n",
    "    # ax[overlay_row][-1].title.set_text('All overlays stacked and clipped to [0..1]')\n",
    "\n",
    "    # # impose the overlay on top of the base image\n",
    "    # ax[superimposed_row][-1].imshow(superimpose(base_image_255, overlay, strategy))  # show the superimposed image on the second row\n",
    "\n",
    "    if save_file is not None:\n",
    "        fig.savefig(save_file, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKlmlwkvSBUM"
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "# average value counter\n",
    "class AVG:\n",
    "    \"\"\"This counter should work for scalars and for torch.Tensors (maybe even for numpy.ndarrays)\n",
    "    \"\"\"\n",
    "    sum_: Union[float, torch.Tensor]\n",
    "    count: Union[float, torch.Tensor]\n",
    "    value: Union[float, torch.Tensor]\n",
    "    def __init__(self, sum_=None, count=None):\n",
    "        self.sum_ = sum_\n",
    "        self.count = count\n",
    "        self.value = None\n",
    "    \n",
    "    def record(self, value, weight=1):\n",
    "        if self.sum_ is None:\n",
    "            self.sum_ = value\n",
    "            self.count = weight\n",
    "        else:\n",
    "            self.sum_ += value\n",
    "            self.count += weight\n",
    "        self.value = None\n",
    "\n",
    "    def __call__(self):\n",
    "        if self.value is None:\n",
    "            self.value = self.sum_ / self.count\n",
    "        return self.value\n",
    "        \n",
    "device_ = 0\n",
    "# set the evaluation mode\n",
    "grad_cam.eval()\n",
    "grad_cam.cuda(device_)\n",
    "\n",
    "epochs_ = 1\n",
    "max_examples = 3\n",
    "\n",
    "images_list = []\n",
    "activations_list = []\n",
    "gradients_list = []\n",
    "predictions_list = []\n",
    "\n",
    "\n",
    "generator = seq_generator\n",
    "for epoch_ in range(epochs_):\n",
    "    print('Generator has', len(generator), 'examples in this epoch.')\n",
    "    for batch_idx in tqdm(range(min(len(generator), max_examples))):\n",
    "\n",
    "        input_, target = generator[batch_idx + 58]\n",
    "        input_ = input_.type(torch.float)\n",
    "\n",
    "        # transform to a 3 channel image shape expected by pyplot\n",
    "        img = (input_.squeeze().permute([1,2,0]).numpy() + 1.) / 2.\n",
    "        images_list.append(img)\n",
    "        \n",
    "        input_ = input_.cuda(device_)\n",
    "        pred = grad_cam(input_)\n",
    "\n",
    "        # check the model decision\n",
    "        is_cancer = pred > 0.5\n",
    "        is_cancer = is_cancer.cpu()\n",
    "        \n",
    "        if is_cancer == target: \n",
    "            _t = 'T'\n",
    "        else:\n",
    "            _t = 'F'\n",
    "        \n",
    "        _t += 'P' if is_cancer else 'N'\n",
    "        predictions_list.append(_t)\n",
    "        \n",
    "        # get the gradient of the output with respect to the parameters of the model\n",
    "        pred.backward()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # pull the gradients out of the model\n",
    "            gradients = grad_cam.get_activations_gradient().cpu().squeeze(0)\n",
    "            gradients_list.append(gradients)\n",
    "\n",
    "            # get the activations of the last convolutional layer\n",
    "            activations = grad_cam.get_activations().cpu().squeeze(0)\n",
    "            activations_list.append(activations)\n",
    "\n",
    "            # # pool the gradients across the feature maps and batch\n",
    "            # pooled_gradients = torch.mean(gradients, dim=[2, 3])\n",
    "            # #pooled_gradients = torch.amax(gradients, dim=(0, 2, 3))\n",
    "            \n",
    "            \n",
    "            # # get the sorted indices of the gradients flowing back\n",
    "            # sorted_gradient_indices = torch.argsort(pooled_gradients, dim=-1, descending=True).numpy()\n",
    "            \n",
    "            # # weight the channels by corresponding gradients through broadcasting multiplication\n",
    "            # weighted_activations = (activations * pooled_gradients.unsqueeze(1).unsqueeze(2)).numpy()\n",
    "            # activations = activations.numpy()\n",
    "\n",
    "            # # select some interesting weighted activations\n",
    "            # selection = [*sorted_gradient_indices[:10], *sorted_gradient_indices[-10:]]\n",
    "            # heatmaps = activations[selection, :, :]\n",
    "\n",
    "            # # relu on top of the heatmap, expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "            # #heatmaps = np.maximum(heatmaps, 0)\n",
    "            \n",
    "            # # row_mins = heatmaps.min(axis=(1, 2), keepdims=True)\n",
    "\n",
    "            # # # normalize all at once\n",
    "            # # row_maxs = heatmaps.max(axis=(1, 2), keepdims=True)\n",
    "            \n",
    "            # # nonzero_idx = (row_maxs != 0).squeeze()\n",
    "            # # heatmaps[nonzero_idx] /= row_maxs[nonzero_idx]  # can't avoid division here\n",
    "\n",
    "            # # heatmaps_titles = [f'Scaled by {1/row_maxs[i].item():.2f}, rank {pooled_gradients[selection[i]]:.4E}' if row_maxs[i] > 0 else f'No positive act., rank {pooled_gradients[selection[i]]:.4E}' for i in range(len(selection))]\n",
    "            \n",
    "            # print(heatmaps.shape)\n",
    "\n",
    "            # heatmaps_titles = [f'Transformed with logit, rank {pooled_gradients[selection[i]]:.4E}' for i in range(len(selection))]\n",
    "            # heatmaps = expit(heatmaps)\n",
    "            \n",
    "            # # heatmaps = np.append(heatmaps, np.max(np.sum(weighted_activations[selection]), 0) * (1/weighted_activations[selection].max()))\n",
    "            # # heatmaps_titles.append('Original GradCAM')\n",
    "\n",
    "            \n",
    "            \n",
    "            # with plt.ioff():\n",
    "            #     plot_bitmap_overlays(heatmaps, img, \n",
    "            #         save_file=f'grad_cam_fmwise_special_blaa_{batch_idx}.jpg',\n",
    "            #         colormap_lut=_COLORMAP_GRADIENT_BIPOLAR_GREEN_RED,\n",
    "            #         strategy='black_as_alpha',\n",
    "            #         heatmaps_titles=heatmaps_titles)\n",
    "            #     #plot_bitmap_overlays(heatmaps, img, f'grad_cam_fmwise_special_outl_{batch_idx}.jpg', strategy='outline', heatmaps_titles=heatmaps_titles)\n",
    "                \n",
    "                \n",
    "            #     #plot_bitmap_overlays(heatmaps, img, f'grad_cam_fmwise_ceil_{batch_idx}.jpg', strategy='ceil', heatmaps_titles=heatmaps_titles)\n",
    "            #     # plot_bitmap_overlays(heatmaps, img, f'grad_cam_fmwise_subt_{batch_idx}.png', strategy='sub')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in trange(len(activations_list)):\n",
    "        img = images_list[i]\n",
    "        activations = activations_list[i]\n",
    "        gradients = gradients_list[i]\n",
    "        pred_checked = predictions_list[i]\n",
    "\n",
    "        print(activations.shape, gradients.shape, pred_checked)\n",
    "        \n",
    "        # pool the gradients across the feature maps and batch\n",
    "        pooled_gradients = torch.mean(gradients, dim=[1, 2])\n",
    "        #pooled_gradients = torch.amax(gradients, dim=(0, 2, 3))\n",
    "        \n",
    "        \n",
    "        # get the sorted indices of the gradients flowing back\n",
    "        sorted_gradient_indices = torch.argsort(pooled_gradients, dim=-1, descending=True).numpy()\n",
    "        \n",
    "        \n",
    "        # weight the channels by corresponding gradients through broadcasting multiplication\n",
    "        weighted_activations = (activations * pooled_gradients.unsqueeze(1).unsqueeze(2)).numpy()\n",
    "        activations = activations.numpy()\n",
    "\n",
    "        # select some interesting weighted activations\n",
    "        selection = [*sorted_gradient_indices[:10], *sorted_gradient_indices[-10:]]\n",
    "        heatmaps = activations[selection, :, :]\n",
    "\n",
    "        # relu on top of the heatmap, expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "        #heatmaps = np.maximum(heatmaps, 0)\n",
    "        \n",
    "        # row_mins = heatmaps.min(axis=(1, 2), keepdims=True)\n",
    "\n",
    "        # # normalize all at once\n",
    "        # row_maxs = heatmaps.max(axis=(1, 2), keepdims=True)\n",
    "        \n",
    "        # nonzero_idx = (row_maxs != 0).squeeze()\n",
    "        # heatmaps[nonzero_idx] /= row_maxs[nonzero_idx]  # can't avoid division here\n",
    "\n",
    "        # heatmaps_titles = [f'Scaled by {1/row_maxs[i].item():.2f}, rank {pooled_gradients[selection[i]]:.4E}' if row_maxs[i] > 0 else f'No positive act., rank {pooled_gradients[selection[i]]:.4E}' for i in range(len(selection))]\n",
    "        \n",
    "        print(heatmaps.shape)\n",
    "\n",
    "        heatmaps_titles = [f'Transformed with logit, rank {pooled_gradients[selection[i]]:.4E}' for i in range(len(selection))]\n",
    "        heatmaps = expit(heatmaps)\n",
    "        \n",
    "        # append original gradcam (weighted sum of activations)\n",
    "        relued_gradcam = np.maximum(np.sum(weighted_activations[selection], axis=0, keepdims=True), 0.)\n",
    "        normalized_gradcam = relued_gradcam * 1/relued_gradcam.max()\n",
    "        heatmaps = np.append(heatmaps, normalized_gradcam, axis=0)\n",
    "        \n",
    "        heatmaps_titles.append('Original GradCAM')\n",
    "\n",
    "        \n",
    "        \n",
    "        with plt.ioff():\n",
    "            plot_bitmap_overlays(heatmaps, img, \n",
    "                save_file=f'grad_cam_fmwise_special_blaa_{i}.jpg',\n",
    "                colormap_lut=_COLORMAP_GRADIENT_BIPOLAR_GREEN_RED,\n",
    "                strategy='black_as_alpha',\n",
    "                heatmaps_titles=heatmaps_titles)\n",
    "            #plot_bitmap_overlays(heatmaps, img, f'grad_cam_fmwise_special_outl_{batch_idx}.jpg', strategy='outline', heatmaps_titles=heatmaps_titles)\n",
    "            \n",
    "            \n",
    "            #plot_bitmap_overlays(heatmaps, img, f'grad_cam_fmwise_ceil_{batch_idx}.jpg', strategy='ceil', heatmaps_titles=heatmaps_titles)\n",
    "            # plot_bitmap_overlays(heatmaps, img, f'grad_cam_fmwise_subt_{batch_idx}.png', strategy='sub')\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prunning_gradcam.models import GMaxPool2d\n",
    "\n",
    "from prunning_gradcam.grad_cam_tools import _load_params, dissect_sequential_model\n",
    "\n",
    "inp = torch.as_tensor([[\n",
    "    [\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 1, 0],\n",
    "        [0, 1, 1, 1, 0],\n",
    "        [0, 1, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0],\n",
    "    ]\n",
    "]], dtype=torch.float32)\n",
    "inp2 = torch.as_tensor([[\n",
    "    [\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0],\n",
    "        [1, 1, 1, 0, 0],\n",
    "        [1, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 0],\n",
    "    ]\n",
    "]], dtype=torch.float32)\n",
    "print(inp.size())\n",
    "state_dict_ = {\n",
    "    '0.weight': torch.as_tensor([[\n",
    "        [[1, 0, 1],\n",
    "         [0, 0, 0],\n",
    "         [1, 0, 1]]\n",
    "    ],[\n",
    "        [[0, 0, 0],\n",
    "         [1, 1, 1],\n",
    "         [0, 0, 0]]\n",
    "    ],[\n",
    "        [[0, 1, 0],\n",
    "         [0, 1, 0],\n",
    "         [0, 1, 0]]\n",
    "    ]], dtype=torch.float32),\n",
    "    '0.bias': torch.as_tensor([ -3, -2, -2], dtype=torch.float32),\n",
    "    # '3.weight': torch.as_tensor([[\n",
    "    #     [[1, 1],\n",
    "    #      [1, 1]],\n",
    "    #     [[1, 1],\n",
    "    #      [1, 1]],\n",
    "    #     [[1, 1],\n",
    "    #      [1, 1]]\n",
    "    # ],[\n",
    "    #     [[0, 0],\n",
    "    #      [1, 1]],\n",
    "    #     [[0, 0],\n",
    "    #      [1, 1]],\n",
    "    #     [[0, 0],\n",
    "    #      [1, 1]]\n",
    "    # ],[\n",
    "    #     [[1, 1],\n",
    "    #      [0, 0]],\n",
    "    #     [[1, 1],\n",
    "    #      [0, 0]],\n",
    "    #     [[1, 1],\n",
    "    #      [0, 0]]\n",
    "    # ]], dtype=torch.float32),\n",
    "    # '3.bias' : torch.as_tensor([0, 0, 0]),\n",
    "    '3.weight': torch.as_tensor([[1, 1,1]], dtype=torch.float32),\n",
    "    '3.bias': torch.as_tensor([1], dtype=torch.float32)\n",
    "}\n",
    "\n",
    "test_model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    # nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    # nn.Conv2d(in_channels=3, out_channels=3, kernel_size=2, padding=0),\n",
    "    # nn.ReLU(inplace=True),\n",
    "    GMaxPool2d(),\n",
    "    nn.Linear(in_features=3, out_features=1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "path_ = 'temp_test_state_dict'\n",
    "torch.save(state_dict_, path_)\n",
    "\n",
    "_load_params(test_model, path_)\n",
    "\n",
    "first, after = dissect_sequential_model(test_model, 1)\n",
    "after_no_sig, sigm = dissect_sequential_model(after, -1)\n",
    " \n",
    "gmodel = GradCam(first, after_no_sig)\n",
    "pred = gmodel(inp)\n",
    "print('PRED', pred)\n",
    "img = inp.squeeze(0).permute([1,2,0]).numpy()\n",
    "        \n",
    "pred.backward()\n",
    "with torch.no_grad():\n",
    "    # pull the gradients out of the model\n",
    "    gradients = gmodel.get_activations_gradient().detach().cpu()\n",
    "    print(\"GRADS\", gradients)\n",
    "\n",
    "    # pool the gradients across the feature maps and batch\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "    #pooled_gradients = torch.amax(gradients, dim=(0, 2, 3))\n",
    "    \n",
    "    \n",
    "    #get the sorted indices of the gradients flowing back\n",
    "    sorted_gradient_indices = torch.argsort(pooled_gradients, dim=-1, descending=True).numpy()\n",
    "    \n",
    "\n",
    "    # get the activations of the last convolutional layer\n",
    "    activations = gmodel.get_activations().detach().cpu().squeeze(0)\n",
    "    print('ACTS', activations)\n",
    "    \n",
    "    # weight the channels by corresponding gradients through broadcasting multiplication\n",
    "    weighted_activations = activations.numpy() # (activations * pooled_gradients.unsqueeze(1).unsqueeze(2)).numpy()\n",
    "    \n",
    "    # select some interesting weighted activations\n",
    "    selection = sorted_gradient_indices[:20]\n",
    "    heatmaps = weighted_activations[selection, :, :]\n",
    "\n",
    "    # relu on top of the heatmap, expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "    heatmaps = np.maximum(heatmaps, 0)\n",
    "\n",
    "    # normalize\n",
    "    row_sums = heatmaps.max(axis=(1, 2), keepdims=True)\n",
    "    nonzero_idx = (row_sums != 0).squeeze()\n",
    "    heatmaps[nonzero_idx] /= row_sums[nonzero_idx]\n",
    "\n",
    "    heatmaps_titles = [f'Scaled by {1/factor}' if factor > 0 else 'Empty' for factor in row_sums]\n",
    "    \n",
    "    with plt.ioff():\n",
    "        plot_bitmap_overlays(heatmaps, img, save_file=\"grad_cam_test.jpg\", strategy='linear_combination', heatmaps_titles=heatmaps_titles)\n",
    "                \n",
    "#print(inp, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grads_for_boxplots = torch.stack(grad_list_fro_boxplot, 0)\n",
    "df = pd.DataFrame(grads_for_boxplots.numpy())\n",
    "sorted_index = df.median().sort_values().index\n",
    "\n",
    "f = plt.figure()\n",
    "f.set_figwidth(128)\n",
    "f.set_figheight(10)\n",
    "\n",
    "sns.boxplot(data=df)\n",
    "plt.ylabel(\"Average gradient size\", size=18)\n",
    "axes = plt.gca()\n",
    "axes.yaxis.grid()\n",
    "plt.savefig(\"Gradients_per_example_sorted_30.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_gradients = torch.as_tensor([0, 5, 2, 3, 4])\n",
    "#pooled_gradients = torch.amax(gradients, dim=(0, 2, 3))\n",
    "\n",
    "\n",
    "#get the sorted indices of the gradients flowing back\n",
    "sorted_gradient_indices = torch.argsort(pooled_gradients, dim=-1, descending=True).numpy()\n",
    "\n",
    "print(pooled_gradients[sorted_gradient_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten = torch.Tensor([\n",
    "    [[[2, 1, 2],\n",
    "     [3, 1, 2],\n",
    "     [4, 1, 2]],\n",
    "     [[5, 1, 2],\n",
    "     [6, 1, 2],\n",
    "     [7, 1, 2]]]\n",
    "    ])\n",
    "print(ten.size())\n",
    "#pls = torch.mean(ten, dim=[0, 2, 3])\n",
    "pls = torch.amax(ten, (0,2,3))\n",
    "pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mba3keNlVvOK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_h5_store_pandas(file_path: str):\n",
    "    store = pd.HDFStore(file_path, mode=\"r\")\n",
    "    return store\n",
    "\n",
    "  \n",
    "#hdfs = load_h5_store_pandas('/mnt/data/home/bajger/NN_pruning/histopat/experiment_output/transfer_learning/predictions.h5')\n",
    "hdfs2 = load_h5_store_pandas('/mnt/data/home/bajger/NN_pruning/histopat/datasets/hdfs_output/hdfs_output.h5')\n",
    "#hdfs3 = load_h5_store_pandas('/mnt/data/crc_ml/data/processed/Prostata/level1/r512px/c256px/t512px/no_overlap/datasets/1602530237.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoEBC1SOaG9S"
   },
   "outputs": [],
   "source": [
    "#print(hdfs3)\n",
    "print(len(hdfs2))\n",
    "index_ = 0\n",
    "for table_name in hdfs2.keys():\n",
    "    print(f'{index_}({len(hdfs2[table_name])})', end=' ')\n",
    "    \n",
    "    if table_name == '/test/TP-2019_2824-01-1':\n",
    "        print(len(hdfs2[table_name]), end=' ')\n",
    "        print(f'The dataset is on the {index_}th position.')\n",
    "        break\n",
    "    index_ += 1 #len(hdfs2[table_name])\n",
    "    #print(hdfs3[table_name])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "GradCAM tinkering for prunning purposes",
   "private_outputs": true,
   "provenance": []
  },
  "interpreter": {
   "hash": "2f826410b06d599b81b1eb559a2f04b87d2afa519d23a7cef8a76b4aca430648"
  },
  "kernelspec": {
   "display_name": "Deep learning kernel",
   "language": "python",
   "name": "my-complete-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
