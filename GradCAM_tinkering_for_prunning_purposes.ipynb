{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rODEqYrP298"
   },
   "source": [
    "Necessary imports for the whole notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6GaHSijPEj6",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtU3Mv9KQ6V-"
   },
   "source": [
    "Load the datagenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATTZHWIwQ412"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from rationai.datagens.generators import BaseGeneratorPytorch\n",
    "from rationai.utils.config import build_from_config, parse_configs_recursively\n",
    "\n",
    "\n",
    "with open('prunning_teacher_student/prunning_ts_config.json') as ts_config_file:\n",
    "    ts_config = json.load(ts_config_file)\n",
    "\n",
    "\n",
    "datagen_bool = build_from_config(parse_configs_recursively(\"datagen_bool\", cfg_store=ts_config[\"named_configs\"]))\n",
    "generators_dict_bool = datagen_bool.build_from_template()\n",
    "\n",
    "train_generator_bool = generators_dict_bool['train_gen']\n",
    "valid_generator_bool = generators_dict_bool['valid_gen']\n",
    "\n",
    "datagen_ts = build_from_config(parse_configs_recursively(\"datagen_ts\", cfg_store=ts_config[\"named_configs\"]))\n",
    "generators_dict_ts = datagen_ts.build_from_template()\n",
    "\n",
    "train_generator_ts = generators_dict_ts['train_gen']\n",
    "valid_generator_ts = generators_dict_ts['valid_gen']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_generator_bool.set_batch_size(batch_size)\n",
    "valid_generator_bool.set_batch_size(batch_size)\n",
    "train_generator_ts.set_batch_size(batch_size)\n",
    "valid_generator_ts.set_batch_size(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V72n9YQZPuLo"
   },
   "source": [
    "## Modules setup\n",
    "\n",
    "Define a GradCam module. The module should be able to be used with an arbitrary network topology that includes convolutional layers.\n",
    "The idea of gradcam is that it looks into a model at the particular layer and weights the gradients flowing back from the following layers with the activations of the particular layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzrwVOn2P15Q"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class GradCam(nn.Module):\n",
    "    \"\"\"\n",
    "    This Module performs grad cam computation for a model consisting of two Modules applied in order.\n",
    "    To use it on an arbitrary model, the model has to be manually dissected first\n",
    "    The dissection for a novolutional networs is usually performes in such a way that the convolutional layers are represented\n",
    "    by the first module and the dense layers are left in the second Module\n",
    "    The dissection can be arbitrary for as long as the output of the first model has a shape of [N, C, W, H],\n",
    "    meaning it has to be a batch of size N, each element containing C feature maps of shape W x H (2D matrices)\n",
    "    so that the spatial information can be transformed into a heatmap overlay for the input picture.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_before_cam: nn.Module, model_after_cam: nn.Module, *args, **kwargs):\n",
    "        \"\"\"Initialize the GradCam for a dissected model, split into two parts.\n",
    "        The parts are expected to split at the layer of interest and together have to make the full model.\n",
    "\n",
    "        Args:\n",
    "            model_before_cam (nn.Module): Part of the inspected model containing up to the convolutional layer we want to inspect \n",
    "            model_after_cam (nn.Module): Remaining layers of the inspected model\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model_before_cam = model_before_cam\n",
    "        self.model_after_cam = model_after_cam\n",
    "        \n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "    \n",
    "    # hook for stroing the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # remember the activation\n",
    "        self.activations = self.model_before_cam(x)\n",
    "        \n",
    "        # register the backward hook (we dont need the reference to it)\n",
    "        _ = self.activations.register_hook(self.activations_hook)\n",
    "        \n",
    "        # apply the remaining layers\n",
    "        return self.model_after_cam(self.activations)\n",
    "        \n",
    "    # getter fro the stored gradients\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # getter for the stored activations\n",
    "    def get_activations(self, x):\n",
    "        return self.activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier, the GradCam module is written in such a way that it takes two modules on initialisation, which represent the two parts of the already dissected model. The model dissection has to be performed manually,\n",
    "though I have provided a method for dissecting any torch.nn.Sequential model on an arbitrary layer index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggEhBL6UnjxY"
   },
   "outputs": [],
   "source": [
    "from prunning_gradcam.models import SequentialVGG16\n",
    "from prunning_gradcam.grad_cam_tools import _load_params, dissect_sequential_model\n",
    "\n",
    "import logging as log\n",
    "\n",
    "# initialize the VGG model\n",
    "vgg = SequentialVGG16()\n",
    "\n",
    "# load weights from a file\n",
    "_load_params(vgg, source_state_dict_path='transplanted-model.chkpt')\n",
    "\n",
    "# dissect model into two parts\n",
    "first_part, remaining_part = dissect_sequential_model(vgg, 30)\n",
    "\n",
    "grad_cam = GradCam(first_part, remaining_part)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation tools\n",
    "\n",
    "For the purpose of visualizing the GradCAM outputs we have to have a way of plotting the heatmaps, overlaying them over the original images or somehow visualizing the activation regions.\n",
    "\n",
    "For that purpose I have coded a function that can overlay several heatmaps over a single image in one step, next to each other in a grid. \n",
    "The last column in the grid shows all the heatmaps over each other clipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "from skimage.morphology import dilation, disk\n",
    "\n",
    "\n",
    "\n",
    "def scaleshift_to_unit_range(img: np.ndarray):\n",
    "    img = img - img.min()\n",
    "    img_max = img.max()\n",
    "    if img_max != 0:\n",
    "        img *= 1/img_max\n",
    "    return img\n",
    "\n",
    "\n",
    "def resize_and_color(bitmap: np.ndarray, dims:Tuple[int]) -> np.ndarray:\n",
    "    overlay = cv2.resize(bitmap, dims)\n",
    "    overlay = np.uint8(255 * overlay)\n",
    "    overlay = cv2.cvtColor(overlay, cv2.COLOR_GRAY2RGB)\n",
    "    overlay = cv2.LUT(overlay, _COLORMAP_JETFROMBLACK)\n",
    "    return overlay\n",
    "\n",
    "\n",
    "def superimpose(image: np.ndarray, overlay: np.ndarray, strategy:str='ceil'):\n",
    "    if strategy=='ceil':\n",
    "        res = np.minimum(image + overlay*0.375, 255)\n",
    "    elif strategy=='sub':\n",
    "        res = np.maximum(image - overlay*0.5, 0)\n",
    "    elif strategy=='linear_combination':\n",
    "        res = image*0.5 + overlay*0.5\n",
    "    elif strategy=='outline':\n",
    "        footprint = disk(3)\n",
    "        bool_map = overlay.sum(axis=2) > 0\n",
    "        dilated = dilation(bool_map, footprint)\n",
    "        outline = dilated & ~bool_map\n",
    "        res = image.copy()\n",
    "        # fill the outline with specific color\n",
    "        res[outline] = [0, 0, 0]\n",
    "    else:\n",
    "        raise NotImplementedError(f'There is no strategy with name {strategy}!')\n",
    "\n",
    "    return res.astype(np.uint8)\n",
    "\n",
    "\n",
    "def _COLORMAP_JETFROMBLACK():\n",
    "    lut = np.asarray([[np.ones(3)*i for _ in range(1)] for i in range(256)], dtype=np.uint8)\n",
    "    lut = cv2.applyColorMap(lut, cv2.COLORMAP_JET)\n",
    "    #lut = cv2.cvtColor(lut, cv2.COLOR_BGR2RGB)\n",
    "    beginning = lut[:64, 0].astype(np.float64)\n",
    "    beginning *= np.stack([np.arange(64) * (1/64) for _ in range(3)], axis=1)\n",
    "    lut[:64, 0] = beginning.astype(np.uint8)\n",
    "    return lut\n",
    "\n",
    "\n",
    "def plot_bitmap_overlays(bitmaps: List[np.ndarray], base_image: np.ndarray, save_file:str=None, strategy='outline', heatmaps_titles=None):\n",
    "    \"\"\"\n",
    "    Creates a grid plot from a list of overlays and a base image.\n",
    "    Expects the overlay arrays in the list to be of shape [W1, H1] and the image of shape [3, W2, H2],\n",
    "    where W1, H1, W2, H2 are widths and heights, not required to be of same size. Overlays are stretched to the image size.\n",
    "    \n",
    "    There are several rows plotted, leftmost column contain teh base images, rightmost column contains all overlays.\n",
    "    Images in in columns in between represent each overlay separately\n",
    "\n",
    "    Args:\n",
    "        bitmaps (List[np.ndarray]): The bitmaps that are going to be transformed into overlays\n",
    "        base_image (np.ndarray): The underlying image\n",
    "        save_file (str, optional): If there is a string present, the figure is saved into a file. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, ncols=len(bitmaps)+2, sharex=True, sharey=True, figsize=(5*(len(bitmaps)+2), 5*2))\n",
    "   \n",
    "    # mark the row indices for flexibility\n",
    "    overlay_row = 0\n",
    "    superimposed_row = 1\n",
    "\n",
    "    # in the first column, just print the original images. The upper is clipped, the lower is rescaled, both to [0..1]\n",
    "    ax[overlay_row][0].imshow(np.maximum(base_image, 0))\n",
    "    ax[overlay_row][0].title.set_text('Input image clipped to [0..1]')\n",
    "    base_image = scaleshift_to_unit_range(base_image)\n",
    "    ax[superimposed_row][0].imshow(base_image)\n",
    "    ax[superimposed_row][0].title.set_text('Input image rescaled to [0..1]')\n",
    "\n",
    "    base_image_255 = (base_image*255)#.astype(np.uint8)\n",
    "    \n",
    "    # for each bitmaps get the overlays and show them\n",
    "    for column, bitmap in enumerate(iterable=bitmaps, start=1):\n",
    "        overlay = resize_and_color(bitmap, base_image.shape[:2])\n",
    "        #print('OVERLAY MAXMIN', overlay.dtype, overlay.max(), overlay.min())\n",
    "        ax[overlay_row][column].imshow(overlay)  # show the separate overlay on the first row\n",
    "        if heatmaps_titles is not None:\n",
    "            ax[overlay_row][column].title.set_text(heatmaps_titles[column-1])\n",
    "\n",
    "        # impose the overlay on top of the base image\n",
    "        ax[superimposed_row][column].imshow(superimpose(base_image_255, overlay, strategy))  # show the superimposed image on the second row\n",
    "        \n",
    "    # get the total averaged overlay from all the bitmaps\n",
    "    overlay = resize_and_color(np.minimum(sum(bitmaps), 1.0), base_image.shape[:2])\n",
    "    ax[overlay_row][-1].imshow(overlay)  # show the separate overlay on the first row\n",
    "    ax[overlay_row][-1].title.set_text('All overlays stacked and clipped to [0..1]')\n",
    "\n",
    "    # impose the overlay on top of the base image\n",
    "    ax[superimposed_row][-1].imshow(superimpose(base_image_255, overlay, strategy))  # show the superimposed image on the second row\n",
    "\n",
    "    if save_file is not None:\n",
    "        fig.savefig(save_file, bbox_inches='tight')\n",
    "    else:\n",
    "        fig.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# precomputed stuff like color LUTs and so on\n",
    "_COLORMAP_JETFROMBLACK = _COLORMAP_JETFROMBLACK()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKlmlwkvSBUM"
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "# average value counter\n",
    "class AVG:\n",
    "    \"\"\"This counter should work for scalars and for torch.Tensors (maybe even for numpy.ndarrays)\n",
    "    \"\"\"\n",
    "    sum_: Union[float, torch.Tensor]\n",
    "    count: Union[float, torch.Tensor]\n",
    "    value: Union[float, torch.Tensor]\n",
    "    def __init__(self, sum_=None, count=None):\n",
    "        self.sum_ = sum_\n",
    "        self.count = count\n",
    "        self.value = None\n",
    "    \n",
    "    def record(self, value, weight=1):\n",
    "        if self.sum_ is None:\n",
    "            self.sum_ = value\n",
    "            self.count = weight\n",
    "        else:\n",
    "            self.sum_ += value\n",
    "            self.count += weight\n",
    "        self.value = None\n",
    "\n",
    "    def __call__(self):\n",
    "        if self.value is None:\n",
    "            self.value = self.sum_ / self.count\n",
    "        return self.value\n",
    "        \n",
    "# set the evaluation mode\n",
    "grad_cam.eval()\n",
    "grad_cam.cuda(1)\n",
    "\n",
    "epochs_ = 1\n",
    "max_examples = 3\n",
    "\n",
    "accuracy_avg = AVG()\n",
    "gradients_avg = AVG()\n",
    "\n",
    "\n",
    "generator = train_generator_bool\n",
    "for epoch_ in range(epochs_):\n",
    "    print('Generator has', len(generator), 'examples in this epoch.')\n",
    "    for batch_idx in tqdm(range(min(len(generator), max_examples))):\n",
    "\n",
    "        input_, target = generator[batch_idx + 200]\n",
    "        input_ = input_.type(torch.float)\n",
    "\n",
    "        # transform to a 3 channel image shape expected by pyplot\n",
    "        img = input_.squeeze().permute([1,2,0]).numpy()\n",
    "        \n",
    "        input_ = input_.cuda(1)\n",
    "        pred = grad_cam(input_)\n",
    "\n",
    "        # check the model decision\n",
    "        is_cancer = pred > 0.5\n",
    "        is_cancer = is_cancer.cpu()\n",
    "        \n",
    "        if is_cancer == target:\n",
    "            accuracy_avg.record(1)\n",
    "            print(\"Correctly predicted:\", is_cancer)\n",
    "        else:\n",
    "            accuracy_avg.record(0)\n",
    "            print(\"Wrongly predicted:\", is_cancer)\n",
    "        \n",
    "        \n",
    "        # get the gradient of the output with respect to the parameters of the model\n",
    "        pred.backward()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # pull the gradients out of the model\n",
    "            gradients = grad_cam.get_activations_gradient().detach().cpu()\n",
    "\n",
    "            # pool the gradients across the feature maps and batch\n",
    "            #pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "            pooled_gradients = torch.amax(gradients, dim=(0, 2, 3))\n",
    "            \n",
    "            \n",
    "            #get the sorted indices of the gradients flowing back\n",
    "            sorted_gradient_indices = torch.argsort(pooled_gradients, dim=-1, descending=True).numpy()\n",
    "            \n",
    "\n",
    "            # get the activations of the last convolutional layer\n",
    "            activations = grad_cam.get_activations(input_).detach().cpu().squeeze(0)\n",
    "            \n",
    "            # weight the channels by corresponding gradients through broadcasting multiplication\n",
    "            weighted_activations = (activations * pooled_gradients.unsqueeze(1).unsqueeze(2)).numpy()\n",
    "            \n",
    "            # select some interesting weighted activations\n",
    "            selection = sorted_gradient_indices[:20]\n",
    "            heatmaps = weighted_activations[selection, :, :]\n",
    "\n",
    "            # relu on top of the heatmap, expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "            heatmaps = np.maximum(heatmaps, 0)\n",
    "\n",
    "            # normalize\n",
    "            row_sums = heatmaps.max(axis=(1, 2), keepdims=True)\n",
    "            nonzero_idx = (row_sums != 0).squeeze()\n",
    "            heatmaps[nonzero_idx] /= row_sums[nonzero_idx]\n",
    "\n",
    "            heatmaps_titles = [f'Scaled by {1/factor}' if factor > 0 else 'Empty' for factor in row_sums]\n",
    "            \n",
    "            with plt.ioff():\n",
    "                plot_bitmap_overlays(heatmaps, img, f'grad_cam_fmwise_linc_{batch_idx}.jpg', strategy='linear_combination', heatmaps_titles=heatmaps_titles)\n",
    "                plot_bitmap_overlays(heatmaps, img, f'grad_cam_fmwise_outl_{batch_idx}.jpg', strategy='outline', heatmaps_titles=heatmaps_titles)\n",
    "                plot_bitmap_overlays(heatmaps, img, f'grad_cam_fmwise_ceil_{batch_idx}.jpg', strategy='ceil', heatmaps_titles=heatmaps_titles)\n",
    "                # plot_bitmap_overlays(heatmaps, img, f'grad_cam_fmwise_subt_{batch_idx}.png', strategy='sub')\n",
    "                \n",
    "\n",
    "    print(accuracy_avg())\n",
    "    \n",
    "\n",
    "# poslat jednotlivé feature mapy s největším rankem vedle sebe\n",
    "# vytvořil překryv jednotlivých vrstev pro kontrolu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "grads_for_boxplots = torch.stack(grad_list_fro_boxplot, 0)\n",
    "df = pd.DataFrame(grads_for_boxplots.numpy())\n",
    "sorted_index = df.median().sort_values().index\n",
    "\n",
    "f = plt.figure()\n",
    "f.set_figwidth(128)\n",
    "f.set_figheight(10)\n",
    "\n",
    "sns.boxplot(data=df)\n",
    "plt.ylabel(\"Average gradient size\", size=18)\n",
    "axes = plt.gca()\n",
    "axes.yaxis.grid()\n",
    "plt.savefig(\"Gradients_per_example_sorted_30.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nums = np.uint8(np.asarray([50, -25, -7, 50, 50])) - np.asarray([0.6863, 0.2846, 1.987,0,0]) \n",
    "nums.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten = torch.Tensor([\n",
    "    [[[2, 1, 2],\n",
    "     [3, 1, 2],\n",
    "     [4, 1, 2]],\n",
    "     [[5, 1, 2],\n",
    "     [6, 1, 2],\n",
    "     [7, 1, 2]]]\n",
    "    ])\n",
    "print(ten.size())\n",
    "#pls = torch.mean(ten, dim=[0, 2, 3])\n",
    "pls = torch.amax(ten, (0,2,3))\n",
    "pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mba3keNlVvOK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_h5_store_pandas(file_path: str):\n",
    "    store = pd.HDFStore(file_path, mode=\"r\")\n",
    "    return store\n",
    "\n",
    "  \n",
    "hdfs = load_h5_store_pandas('/mnt/data/home/bajger/NN_pruning/histopat/experiment_output/transfer_learning/predictions.h5')\n",
    "hdfs2 = load_h5_store_pandas('/mnt/data/home/bajger/NN_pruning/histopat/datasets/hdfs_output/hdfs_output.h5')\n",
    "i = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoEBC1SOaG9S"
   },
   "outputs": [],
   "source": [
    "for table_name in hdfs.keys():\n",
    "    print(hdfs2[table_name])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "GradCAM tinkering for prunning purposes",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
