{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"1\"\n",
    "import glob\n",
    "import openslide\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Queue, Process\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)\n",
    "\n",
    "def chunker(seq, size):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def divide_round_up(n, d):\n",
    "        return (n + (d - 1))//d    \n",
    "\n",
    "def build_model():\n",
    "    inputs = tf.keras.layers.Input((96,96,3))\n",
    "    nasnet_model = tf.keras.applications.nasnet.NASNetMobile(include_top=False, input_tensor=inputs, weights='imagenet')\n",
    "    nasnet_model.trainable=True\n",
    "\n",
    "    x = nasnet_model(inputs)\n",
    "    out1 = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "    out2 = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    out3 = tf.keras.layers.Flatten()(x)\n",
    "    out = tf.keras.layers.Concatenate(axis=-1)([out1, out2, out3])\n",
    "    out = tf.keras.layers.Dropout(0.5)(out)\n",
    "    out = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(out)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs, out)\n",
    "    \n",
    "    loss_fn = tf.keras.losses.binary_crossentropy\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(), \n",
    "            tf.keras.metrics.Precision(), \n",
    "            tf.keras.metrics.Recall()]\n",
    "    \n",
    "    model.compile(loss=loss_fn, metrics=metrics, optimizer=tf.keras.optimizers.Adam(lr=0.0001))\n",
    "    \n",
    "    return model\n",
    "model = build_model()\n",
    "    \n",
    "def get_augmenter():\n",
    "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    seq = iaa.Sequential(\n",
    "        [\n",
    "            # apply the following augmenters to most images\n",
    "            iaa.Fliplr(0.5),  # horizontally flip 50% of all images\n",
    "            iaa.Flipud(0.2),  # vertically flip 20% of all images\n",
    "            sometimes(iaa.Affine(\n",
    "                scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
    "                # scale images to 80-120% of their size, individually per axis\n",
    "                translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},  # translate by -20 to +20 percent (per axis)\n",
    "                rotate=(-10, 10),  # rotate by -45 to +45 degrees\n",
    "                shear=(-5, 5),  # shear by -16 to +16 degrees\n",
    "                order=[0, 1],  # use nearest neighbour or bilinear interpolation (fast)\n",
    "                cval=(0, 255),  # if mode is constant, use a cval between 0 and 255\n",
    "                mode=ia.ALL  # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "            )),\n",
    "            # execute 0 to 5 of the following (less important) augmenters per image\n",
    "            # don't execute all of them, as that would often be way too strong\n",
    "            iaa.SomeOf((0, 5),\n",
    "                    [\n",
    "                        sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))),\n",
    "                        # convert images into their superpixel representation\n",
    "                        iaa.OneOf([\n",
    "                            iaa.GaussianBlur((0, 1.0)),  # blur images with a sigma between 0 and 3.0\n",
    "                            iaa.AverageBlur(k=(3, 5)),\n",
    "                            # blur image using local means with kernel sizes between 2 and 7\n",
    "                            iaa.MedianBlur(k=(3, 5)),\n",
    "                            # blur image using local medians with kernel sizes between 2 and 7\n",
    "                        ]),\n",
    "                        iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)),  # sharpen images\n",
    "                        iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),  # emboss images\n",
    "                        # search either for all edges or for directed edges,\n",
    "                        # blend the result with the original image using a blobby mask\n",
    "                        iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
    "                            iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
    "                            iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n",
    "                        ])),\n",
    "                        iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01 * 255), per_channel=0.5),\n",
    "                        # add gaussian noise to images\n",
    "                        iaa.OneOf([\n",
    "                            iaa.Dropout((0.01, 0.05), per_channel=0.5),  # randomly remove up to 10% of the pixels\n",
    "                            iaa.CoarseDropout((0.01, 0.03), size_percent=(0.01, 0.02), per_channel=0.2),\n",
    "                        ]),\n",
    "                        iaa.Invert(0.01, per_channel=True),  # invert color channels\n",
    "                        iaa.Add((-2, 2), per_channel=0.5),\n",
    "                        # change brightness of images (by -10 to 10 of original value)\n",
    "                        iaa.AddToHueAndSaturation((-1, 1)),  # change hue and saturation\n",
    "                        # either change the brightness of the whole image (sometimes\n",
    "                        # per channel) or change the brightness of subareas\n",
    "                        iaa.OneOf([\n",
    "                            iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
    "                            iaa.FrequencyNoiseAlpha(\n",
    "                                exponent=(-1, 0),\n",
    "                                first=iaa.Multiply((0.9, 1.1), per_channel=True),\n",
    "                                second=iaa.ContrastNormalization((0.9, 1.1))\n",
    "                            )\n",
    "                        ]),\n",
    "                        sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)),\n",
    "                        # move pixels locally around (with random strengths)\n",
    "                        sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))),\n",
    "                        # sometimes move parts of the image around\n",
    "                        sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
    "                    ],\n",
    "                    random_order=True\n",
    "                    )\n",
    "        ],\n",
    "        random_order=True\n",
    "    )\n",
    "    return seq\n",
    "seq = get_augmenter()\n",
    "\n",
    "def sequential_batch_generator(pd_data, batch_size):\n",
    "        while True:                \n",
    "            for batch in chunker(pd_data, 32):\n",
    "                data = [tf.keras.applications.nasnet.preprocess_input(x) for x in batch['image']]\n",
    "                yield np.array(data), np.array(batch['class']).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLIDE_DIR = '/mnt/data/scans/AI scans/Mammy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_slide_name_from_filename(filename, context=False):\n",
    "    \"\"\" GET SLIDE NAME FROM PANDAS DATAFRAME FILENAME \"\"\"\n",
    "    parts = 4\n",
    "    if context:\n",
    "        parts = 5\n",
    "    return os.path.splitext(os.path.basename(filename))[0].rsplit('-', parts)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slides(context_size):\n",
    "    \"\"\" RETURN LIST OF PANDAS SLIDE COORDINATES \"\"\"\n",
    "    MAP_DIR = '/home/matejg/wsi_maps/Mammy/level0/'\n",
    "    if not context_size or context_size in [0,1]:\n",
    "        return glob.glob(MAP_DIR + 'normal/*.gz')\n",
    "    else:\n",
    "        return glob.glob(MAP_DIR + 'macro/*context{}.gz'.format(context_size))\n",
    "    \n",
    "def open_all_slides(slides, context=False):\n",
    "    \"\"\" OPEN EVERY SLIDE ON THE DISK \"\"\"\n",
    "    open_slides = {}\n",
    "    for slide in slides:\n",
    "        slide_name = extract_slide_name_from_filename(slide, context)\n",
    "        slide_fn = SLIDE_DIR + slide_name + '.mrxs'\n",
    "        open_slides[slide_name] = openslide.open_slide(slide_fn)\n",
    "    return open_slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_slides(slides, ratio=0.15):\n",
    "    \"\"\" SPLIT SLIDES INTO TRAIN, VALID, TEST\"\"\"\n",
    "    test_size = int(len(slides) * ratio)\n",
    "    train_val, test = train_test_split(slides, test_size=test_size)\n",
    "    train, valid = train_test_split(train_val, test_size=test_size)\n",
    "    return train, valid, test\n",
    "\n",
    "def get_sampler(slides, context=False):\n",
    "    \"\"\" PREPARE SAMPLER \"\"\"\n",
    "    sampler = {'normal': {}, 'cancer': {}}\n",
    "    for slide in slides:\n",
    "        slide_name = extract_slide_name_from_filename(slide, context)\n",
    "        df = pd.read_pickle(slide)\n",
    "        sampler['normal'][slide_name] = df[df['class'] == 0]['class']\n",
    "        sampler['cancer'][slide_name] = df[df['class'] == 1]['class']\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_epoch_coords(sampler, size):\n",
    "    \"\"\" GENERATE AMOUNT OF UNIQUE TRAIN SAMPLES \"\"\"\n",
    "    static_sample = defaultdict(set)\n",
    "    for no in range(size):\n",
    "        if (no+1) % 5000 == 0:\n",
    "            print('{:,}\\r'.format(no+1), end='')\n",
    "        slide_name, coord, label = sample(sampler)\n",
    "        while coord in static_sample[slide_name]:\n",
    "            slide_name, coord, label = sample(sampler)\n",
    "        static_sample[slide_name].add((coord, label))\n",
    "    return static_sample\n",
    "\n",
    "def sample(sampler):\n",
    "    \"\"\" SAMPLE SINGLE VALID COORDINATE ACCORDING TO SAMPLING SCHEME\"\"\"\n",
    "    label = np.random.choice(list(sampler.keys()))\n",
    "    slide_name = np.random.choice(list(sampler[label].keys()))\n",
    "    row_idx = np.random.randint(low=0, high=len(sampler[label][slide_name]))\n",
    "    row = sampler[label][slide_name].iloc[[row_idx]]\n",
    "    return slide_name, row.index[0], row.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_epoch_tiles(static_sample):\n",
    "    \"\"\" EXTRACT TILES FROM EACH SLIDE IN AN EFFICIENT MANNER \"\"\"\n",
    "    static_tiles = {'image': [], 'class': []}\n",
    "    for idx, (k, v) in enumerate(static_sample.items()):\n",
    "        print('{} {}/{}\\r'.format(k, idx+1, len(static_sample)), end='')\n",
    "        v = sorted(v)\n",
    "        for coord, label in v:\n",
    "            static_tiles['image'].append(np.array(extract_tile(k, coord).convert('RGB')))\n",
    "            static_tiles['class'].append(label)\n",
    "    return static_tiles\n",
    "\n",
    "def extract_tile(slide_name, col_row):\n",
    "    \"\"\" EXTRACT SINGLE TILE FROM A SLIDE \"\"\"\n",
    "    CENTER_SIZE = 32\n",
    "    TILE_SIZE = 96\n",
    "    \n",
    "    col, row = col_row\n",
    "    x_coord = (col-1)*CENTER_SIZE\n",
    "    y_coord = (row-1)*CENTER_SIZE    \n",
    "    return open_slides[slide_name].read_region(location=(x_coord, y_coord), level=(2), size=(TILE_SIZE,TILE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_pd(epoch_size, batch_size, queue):\n",
    "    \"\"\" PROCESS TO BE RUN TO PREFETCH DATA \"\"\"\n",
    "    steps_per_epoch = divide_round_up(epoch_size, batch_size)\n",
    "    train_epoch_coords = generate_train_epoch_coords(epoch_size)\n",
    "    train_tiles_img = extract_epoch_tiles(train_epoch_coords)\n",
    "    train_tiles_pd = pd.DataFrame.from_dict(train_tiles_img).sample(frac=1).reset_index(drop=True)\n",
    "    if queue:\n",
    "        queue.put((train_tiles_pd, steps_per_epoch))\n",
    "        print('--- FINISHED QUEUE PACK ---')\n",
    "    else:\n",
    "        return train_tiles_pd, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_tiles_pd, batch_size, steps_per_epoch):\n",
    "    \"\"\" RUN TRAINING \"\"\"\n",
    "    %time _ = model.fit_generator(sequential_batch_generator(train_tiles_pd, batch_size), steps_per_epoch=steps_per_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== EPOCH #0 ==\n",
      "--- FINISHED QUEUE PACK ---\n",
      "--- FINISHED QUEUE PACK ---\n",
      "625/625 [==============================] - 182s 291ms/step - loss: 0.1472 - binary_accuracy: 0.9459 - precision: 0.9397 - recall: 0.9525\n",
      "== EPOCH #1 ==\n",
      "--- FINISHED QUEUE PACK ---............] - ETA: 1:48 - loss: 0.0292 - binary_accuracy: 0.9906 - precision: 0.9896 - recall: 0.99165,000\n",
      "625/625 [==============================] - 180s 288ms/step - loss: 0.0217 - binary_accuracy: 0.9931 - precision: 0.9928 - recall: 0.9935\n",
      "== EPOCH #2 ==\n",
      "--- FINISHED QUEUE PACK ---............] - ETA: 1:48 - loss: 0.0083 - binary_accuracy: 0.9974 - precision: 0.9983 - recall: 0.99655,000\n",
      "625/625 [==============================] - 178s 284ms/step - loss: 0.0138 - binary_accuracy: 0.9949 - precision: 0.9952 - recall: 0.9946\n",
      "== EPOCH #3 ==\n",
      "--- FINISHED QUEUE PACK ---............] - ETA: 1:49 - loss: 0.0189 - binary_accuracy: 0.9923 - precision: 0.9917 - recall: 0.99275,000\n",
      "625/625 [==============================] - 181s 289ms/step - loss: 0.0218 - binary_accuracy: 0.9916 - precision: 0.9918 - recall: 0.9915\n",
      "== EPOCH #4 ==\n",
      "--- FINISHED QUEUE PACK ---............] - ETA: 1:53 - loss: 0.0260 - binary_accuracy: 0.9908 - precision: 0.9896 - recall: 0.99195,000\n",
      "625/625 [==============================] - 187s 299ms/step - loss: 0.0258 - binary_accuracy: 0.9903 - precision: 0.9891 - recall: 0.9915\n",
      "== EPOCH #5 ==\n",
      "--- FINISHED QUEUE PACK ---............] - ETA: 1:56 - loss: 0.0168 - binary_accuracy: 0.9935 - precision: 0.9935 - recall: 0.99335,000\n",
      "625/625 [==============================] - 188s 300ms/step - loss: 0.0206 - binary_accuracy: 0.9930 - precision: 0.9931 - recall: 0.9930\n",
      "== EPOCH #6 ==\n",
      "--- FINISHED QUEUE PACK ---............] - ETA: 1:56 - loss: 0.0168 - binary_accuracy: 0.9940 - precision: 0.9943 - recall: 0.99355,000\n",
      "625/625 [==============================] - 187s 300ms/step - loss: 0.0178 - binary_accuracy: 0.9937 - precision: 0.9938 - recall: 0.9936\n",
      "== EPOCH #7 ==\n",
      "--- FINISHED QUEUE PACK ---............] - ETA: 1:53 - loss: 0.0120 - binary_accuracy: 0.9948 - precision: 0.9952 - recall: 0.99445,000\n",
      "625/625 [==============================] - 186s 297ms/step - loss: 0.0154 - binary_accuracy: 0.9944 - precision: 0.9946 - recall: 0.9942\n",
      "== EPOCH #8 ==\n",
      "--- FINISHED QUEUE PACK ---............] - ETA: 1:51 - loss: 0.0161 - binary_accuracy: 0.9950 - precision: 0.9945 - recall: 0.99555,000\n",
      "625/625 [==============================] - 184s 295ms/step - loss: 0.0129 - binary_accuracy: 0.9957 - precision: 0.9954 - recall: 0.9960\n",
      "== EPOCH #9 ==\n",
      "--- FINISHED QUEUE PACK ---............] - ETA: 1:54 - loss: 0.0107 - binary_accuracy: 0.9958 - precision: 0.9954 - recall: 0.99625,000\n",
      "625/625 [==============================] - 188s 300ms/step - loss: 0.0156 - binary_accuracy: 0.9947 - precision: 0.9944 - recall: 0.9950\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\" PREPARING NEXT EPOCH DURING TRAINING - SINGLE TILE LEARNING \"\"\"\n",
    "    BATCH_SIZE = 32\n",
    "    TOTAL_EPOCHS = 3\n",
    "    EPOCH_SIZE = 20000\n",
    "\n",
    "    q = Queue(2)\n",
    "    p = Process(target=get_epoch_pd, args=(EPOCH_SIZE, BATCH_SIZE, q))\n",
    "    p.start()\n",
    "\n",
    "    for epoch_id in range(10):\n",
    "        print('== EPOCH #{} =='.format(epoch_id))\n",
    "        new_p = Process(target=get_epoch_pd, args=(EPOCH_SIZE, BATCH_SIZE, q))\n",
    "        new_p.start()\n",
    "        train_tiles_pd, steps_per_epoch = q.get()\n",
    "        model.fit_generator(sequential_batch_generator(train_tiles_pd, batch_size), steps_per_epoch=steps_per_epoch, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
